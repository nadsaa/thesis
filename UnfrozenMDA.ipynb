{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from monai.networks.blocks.patchembedding import PatchEmbeddingBlock\n",
    "from monai.networks.blocks.transformerblock import TransformerBlock\n",
    "from monai.utils import deprecated_arg\n",
    "from collections.abc import Sequence\n",
    "\n",
    "\n",
    "__all__ = [\"ViT\"]\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT), based on: \"Dosovitskiy et al.,\n",
    "    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>\"\n",
    "\n",
    "    ViT supports Torchscript but only works for Pytorch after 1.8.\n",
    "    \"\"\"\n",
    "\n",
    "    @deprecated_arg(\n",
    "        name=\"pos_embed\", since=\"1.2\", removed=\"1.4\", new_name=\"proj_type\", msg_suffix=\"please use `proj_type` instead.\"\n",
    "    )\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        img_size: Sequence[int] | int,\n",
    "        patch_size: Sequence[int] | int,\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_layers: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"conv\",\n",
    "        proj_type: str = \"conv\",\n",
    "        pos_embed_type: str = \"learnable\",\n",
    "        classification: bool = False,\n",
    "        num_classes: int = 2,\n",
    "        dropout_rate: float = 0.0,\n",
    "        spatial_dims: int = 3,\n",
    "        post_activation=\"Tanh\",\n",
    "        qkv_bias: bool = False,\n",
    "        save_attn: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbeddingBlock(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            proj_type=proj_type,\n",
    "            pos_embed_type=pos_embed_type,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    hidden_size=hidden_size,\n",
    "                    mlp_dim=mlp_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    spatial_dims=spatial_dims,\n",
    "                    post_activation=post_activation,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    save_attn=save_attn,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        if classification:\n",
    "            self.classification_head = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ct = x[:, 0, :, :, :]  # Extract CT scan\n",
    "        pet = x[:, 1, :, :, :]  # Extract PET scan\n",
    "\n",
    "        x_ct = self.patch_embedding(ct)  # Convert CT scan to patch embeddings\n",
    "        x_pet = self.patch_embedding(pet)  # Convert PET scan to patch embeddings\n",
    "\n",
    "        x = torch.cat((x_ct, x_pet), dim=1)  # Concatenate CT and PET patch embeddings\n",
    "\n",
    "        if hasattr(self, \"cls_token\"):\n",
    "            cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        hidden_states_out = []\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "            hidden_states_out.append(x)\n",
    "        x = self.norm(x)\n",
    "        if hasattr(self, \"classification_head\"):\n",
    "            x = self.classification_head(x[:, 0])\n",
    "        return x, hidden_states_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##UNFROZEN DECODER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "# import wandb\n",
    "\n",
    "import monai\n",
    "from monai.losses import DiceCELoss, DiceFocalLoss, FocalLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    MapTransform,\n",
    "    ScaleIntensityd,\n",
    "    #AddChanneld,\n",
    "    SpatialPadd,\n",
    "    CenterSpatialCropd,\n",
    "    EnsureChannelFirstd,\n",
    "    ConcatItemsd,\n",
    "    AdjustContrastd, \n",
    "    Rand3DElasticd,\n",
    "    HistogramNormalized,\n",
    "    NormalizeIntensityd,\n",
    "    Invertd,\n",
    "    SaveImage,\n",
    "\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR, UNETR, SegResNet\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "from monai import data\n",
    "\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "# \n",
    "from unetr import CustomedUNETR\n",
    "from vit import ViT\n",
    "from monai.transforms import Compose\n",
    "#from monai.data import NiftiDataset\n",
    "import json\n",
    "from monai.data import Dataset, DataLoader\n",
    "from torch import device\n",
    "from monai.transforms import AsDiscrete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped'\n",
    "json_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped/MDA_CTPT_TRAIN.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datafold_read(datalist, basedir, fold=0, key=\"training\"):\n",
    "    with open(datalist) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    json_data = json_data[key]\n",
    "\n",
    "    for d in json_data:\n",
    "        for k in d:\n",
    "            if isinstance(d[k], list):\n",
    "                d[k] = [os.path.join(basedir, iv) for iv in d[k]]\n",
    "            elif isinstance(d[k], str):\n",
    "                d[k] = os.path.join(basedir, d[k]) if len(d[k]) > 0 else d[k]\n",
    "\n",
    "    tr = []\n",
    "    val = []\n",
    "    for d in json_data:\n",
    "        if \"fold\" in d and d[\"fold\"] == fold:\n",
    "            val.append(d)\n",
    "        else:\n",
    "            tr.append(d)\n",
    "\n",
    "    return tr, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 44)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files, validation_files = datafold_read(datalist=json_dir, basedir=data_dir, fold=0)\n",
    "len(train_files), len(validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipCT(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on hecktor classes:\n",
    "    label 1 is the tumor\n",
    "    label 2 is the lymph node\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            if key == \"ct\":\n",
    "                d[key] = torch.clip(d[key], min=-200, max=200)\n",
    "            # elif key == \"pt\":\n",
    "            #     d[key] = torch.clip(d[key], d[key].min(), 5)\n",
    "        return d\n",
    "\n",
    "class MulPTFM(MapTransform):\n",
    "    \"\"\"\n",
    "    Mult PT and FM \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "\n",
    "        fm = d[\"ct\"] > 0\n",
    "        d[\"pt\"] = d[\"pt\"] * fm\n",
    "        return d\n",
    "\n",
    "class SelectClass(MapTransform):\n",
    "    \"\"\"\n",
    "    Select the class for which you want to fine tune the model \n",
    "\n",
    "    \"\"\"\n",
    "    # def __init__(self, keys, cls=1):\n",
    "    #     super(self).__init__(keys)\n",
    "    #     self.cls = cls\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        d[\"seg\"][d[\"seg\"] == 1] = 0\n",
    "        # d[\"seg\"][d[\"seg\"] == 2] = 1\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from monai.transforms import EnsureTyped\n",
    "# Path to the JSON file\n",
    "json_file_path = '/home/nada.saadi/MIS-FM/hecktor2022_cropped/MDA_CTPT_TRAIN.json'\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data_json = json.load(file)[\"training\"]\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, Orientationd, NormalizeIntensityd, \n",
    "     ScaleIntensityd, ConcatItemsd, RandCropByPosNegLabeld, \n",
    "    RandFlipd, RandRotate90d, SpatialPadd\n",
    ")\n",
    "\n",
    "# Split data into training and validation based on fold\n",
    "train_data = [entry for entry in data_json if entry['fold'] != 0]  # Using fold 0 for validation\n",
    "val_data = [entry for entry in data_json if entry['fold'] == 0]\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "def create_dataloader(data, transforms, batch_size=2, shuffle=True):\n",
    "    dataset = Dataset(data=data, transform=transforms)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=8)\n",
    "\n",
    "\n",
    "num_samples = 4\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"ct\", \"pt\", \"seg\"], ensure_channel_first=True),\n",
    "        SpatialPadd(keys=[\"ct\", \"pt\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "        Orientationd(keys=[\"ct\", \"pt\", \"seg\"], axcodes=\"PLS\"),\n",
    "        NormalizeIntensityd(keys=[\"pt\"]),\n",
    "        ClipCT(keys=[\"ct\"]),\n",
    "        ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "        ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),  # Concatenate CT and PET\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"ctpt\", \"seg\"],\n",
    "            label_key=\"seg\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=num_samples,\n",
    "            image_key=\"ctpt\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        RandFlipd(keys=[\"ctpt\", \"seg\"], spatial_axis=[0], prob=0.20),\n",
    "        RandFlipd(keys=[\"ctpt\", \"seg\"], spatial_axis=[1], prob=0.20),\n",
    "        RandFlipd(keys=[\"ctpt\", \"seg\"], spatial_axis=[2], prob=0.20),\n",
    "        RandRotate90d(keys=[\"ctpt\", \"seg\"], prob=0.20, max_k=3),\n",
    "        EnsureTyped(keys=[\"ctpt\", \"seg\"]),\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"ct\", \"pt\", \"seg\"], ensure_channel_first=True),\n",
    "        SpatialPadd(keys=[\"ct\", \"pt\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "        Orientationd(keys=[\"ct\", \"pt\", \"seg\"], axcodes=\"PLS\"),\n",
    "        NormalizeIntensityd(keys=[\"pt\"]),\n",
    "        ClipCT(keys=[\"ct\"]),\n",
    "        ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "        ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),# Concatenate CT and PET\n",
    "        EnsureTyped(keys=[\"ctpt\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_loader = create_dataloader(train_data, train_transforms, shuffle=True)\n",
    "val_loader = create_dataloader(val_data, val_transforms, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 152/152 [02:10<00:00,  1.16it/s]\n",
      "Loading dataset: 100%|██████████| 44/44 [00:36<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dir='/home/nada.saadi/CTPET/hecktor2022_cropped/UNFROZEN-PATCH-MDA'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from monai.transforms import Compose, LoadImaged, ScaleIntensityRanged, ConcatItemsd\n",
    "from monai.data import Dataset\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from collections.abc import Sequence\n",
    "from monai.transforms import EnsureTyped\n",
    "\n",
    "\n",
    "from vit import ViT\n",
    "from unetr import CustomedUNETR\n",
    "\n",
    "\n",
    "\n",
    "# from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "# from monai.networks.blocks.unetr_block import UnetrBasicBlock, UnetrPrUpBlock, UnetrUpBlock\n",
    "# from vit import ViT\n",
    "# from monai.utils import deprecated_arg, ensure_tuple_rep\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = '/home/nada.saadi/MIS-FM/hecktor2022_cropped/MDA_CTPT_TRAIN.json'\n",
    "\n",
    "# Load the data from the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data_json = json.load(file)[\"training\"]\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, Orientationd, NormalizeIntensityd, \n",
    "     ScaleIntensityd, ConcatItemsd, RandCropByPosNegLabeld, \n",
    "    RandFlipd, RandRotate90d, SpatialPadd\n",
    ")\n",
    "\n",
    "# # Split data into training and validation based on fold\n",
    "# train_data = [entry for entry in data_json if entry['fold'] != 0]  # Using fold 0 for validation\n",
    "# val_data = [entry for entry in data_json if entry['fold'] == 0]\n",
    "\n",
    "# # Create datasets and dataloaders\n",
    "# def create_dataloader(data, transforms, batch_size=2, shuffle=True):\n",
    "#     dataset = Dataset(data=data, transform=transforms)\n",
    "#     return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=8)\n",
    "\n",
    "\n",
    "num_samples = 4\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"ct\", \"pt\", \"seg\"], ensure_channel_first=True),\n",
    "        SpatialPadd(keys=[\"ct\", \"pt\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "        Orientationd(keys=[\"ct\", \"pt\", \"seg\"], axcodes=\"PLS\"),\n",
    "        NormalizeIntensityd(keys=[\"pt\"]),\n",
    "        ClipCT(keys=[\"ct\"]),\n",
    "        ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "        ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),  # Concatenate CT and PET\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"ctpt\", \"seg\"],\n",
    "            label_key=\"seg\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=num_samples,\n",
    "            image_key=\"ctpt\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        RandFlipd(keys=[\"ctpt\", \"seg\"], spatial_axis=[0], prob=0.20),\n",
    "        RandFlipd(keys=[\"ctpt\", \"seg\"], spatial_axis=[1], prob=0.20),\n",
    "        RandFlipd(keys=[\"ctpt\", \"seg\"], spatial_axis=[2], prob=0.20),\n",
    "        RandRotate90d(keys=[\"ctpt\", \"seg\"], prob=0.20, max_k=3),\n",
    "        EnsureTyped(keys=[\"ctpt\", \"seg\"]),\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"ct\", \"pt\", \"seg\"], ensure_channel_first=True),\n",
    "        SpatialPadd(keys=[\"ct\", \"pt\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "        Orientationd(keys=[\"ct\", \"pt\", \"seg\"], axcodes=\"PLS\"),\n",
    "        NormalizeIntensityd(keys=[\"pt\"]),\n",
    "        ClipCT(keys=[\"ct\"]),\n",
    "        ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "        ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),# Concatenate CT and PET\n",
    "        EnsureTyped(keys=[\"ctpt\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "from monai.data import CacheDataset  # Using CacheDataset for efficiency\n",
    "\n",
    "def create_dataloader(data, transforms, batch_size=2, shuffle=True):\n",
    "    # Reformat the data to match the expected format\n",
    "    formatted_data = [{\"ct\": entry[\"ct\"], \"pt\": entry[\"pt\"], \"seg\": entry[\"seg\"]} for entry in data]\n",
    "\n",
    "    # Create CacheDataset with the reformatted data\n",
    "    dataset = CacheDataset(data=formatted_data, transform=transforms, cache_rate=1.0)\n",
    "\n",
    "    train_data = [entry for entry in data_json if entry['fold'] != 0]  # Using fold 0 for validation\n",
    "    val_data = [entry for entry in data_json if entry['fold'] == 0]\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    def create_dataloader(data, transforms, batch_size=2, shuffle=True):\n",
    "        dataset = Dataset(data=data, transform=transforms)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=8)\n",
    "\n",
    "\n",
    "\n",
    "    # Create DataLoader\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=8)\n",
    "\n",
    "# Create dataloaders using the modified function\n",
    "train_loader = create_dataloader(train_data, train_transforms, shuffle=True)\n",
    "val_loader = create_dataloader(val_data, val_transforms, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, loss, optimizer, and metrics\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Instantiate the model\n",
    "model = UNETR(\n",
    "    in_channels=1,  # Number of input channels\n",
    "    out_channels=3,  # Number of output channels\n",
    "    img_size=(96, 96, 96),  # Size of the input image\n",
    "    feature_size=48,  # Size of the feature maps\n",
    "    hidden_size=768,\n",
    "    num_heads = 12,# Size of the hidden layers in the transformer\n",
    "    mlp_dim=3072,  # Dimension of the MLP in the transformer\n",
    "    pos_embed=\"perceptron\",  # Type of positional embedding\n",
    "    norm_name=\"instance\",  # Type of normalization\n",
    "    res_block=True,  # Whether to use residual blocks\n",
    "    dropout_rate=0.0,\n",
    "    proj_type= \"conv\",\n",
    ").to(device)\n",
    "\n",
    "\n",
    "loss_function = DiceLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "dice_metric_batch = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
    "\n",
    "# Training loop\n",
    "max_num_epochs = 530\n",
    "max_iterations = 18000\n",
    "eval_num = 100\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "\n",
    "for epoch in range(max_num_epochs):\n",
    "    model.train()\n",
    "    for batch_data in train_loader:\n",
    "        # Debugging: Check the type and keys of batch_data\n",
    "        print(\"Batch data type:\", type(batch_data))\n",
    "        if isinstance(batch_data, dict):\n",
    "            print(\"Batch data keys:\", batch_data.keys())\n",
    "\n",
    "        if isinstance(batch_data, dict) and \"ctpt\" in batch_data and \"seg\" in batch_data:\n",
    "            inputs, targets = batch_data[\"ctpt\"].to(device), batch_data[\"seg\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            print(\"Invalid batch format received\")\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_data in val_loader:\n",
    "            val_inputs, val_targets = val_data[\"ctpt\"].to(device), val_data[\"seg\"].to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            dice_metric(y_pred=val_outputs, y=val_targets)\n",
    "            dice_metric_batch(y_pred=val_outputs, y=val_targets)\n",
    "\n",
    "    avg_dice = dice_metric.aggregate().item()\n",
    "    metric_batch = dice_metric_batch.aggregate()\n",
    "    tumor_dice, lymph_dice = metric_batch[0].item(), metric_batch[1].item()\n",
    "    dice_metric.reset()\n",
    "    dice_metric_batch.reset()\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch+1}/{max_num_epochs}, Avg Dice: {avg_dice}, Tumor Dice: {tumor_dice}, Lymph Dice: {lymph_dice}\")\n",
    "\n",
    "    # Save model if it's the best so far\n",
    "    if avg_dice > dice_val_best:\n",
    "        dice_val_best = avg_dice\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir, f\"/home/nada.saadi/CTPET/hecktor2022_cropped/UNFROZEN-PATCH-MDA/Unfrozen-mda-model{epoch+1}.pth\"))\n",
    "\n",
    "    # Update global step\n",
    "    global_step += len(train_loader)\n",
    "    if global_step >= max_iterations:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(in_channels: 'int', out_channels: 'int', img_size: 'Sequence[int] | int', feature_size: 'int' = 16, hidden_size: 'int' = 768, mlp_dim: 'int' = 3072, num_heads: 'int' = 12, pos_embed: 'str' = 'conv', proj_type: 'str' = 'conv', norm_name: 'tuple | str' = 'instance', conv_block: 'bool' = True, res_block: 'bool' = True, dropout_rate: 'float' = 0.0, spatial_dims: 'int' = 3, qkv_bias: 'bool' = False, save_attn: 'bool' = False) -> 'None'\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from monai.networks.nets import UNETR\n",
    "\n",
    "print(inspect.signature(UNETR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_data.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =[1,2,3,4,5,6]\n",
    "a[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'monai.networks.nets.unetr.UNETR'>\n"
     ]
    }
   ],
   "source": [
    "import monai.networks.nets as nets\n",
    "print(nets.UNETR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
