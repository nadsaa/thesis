{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413136/306556414.py:9: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from collections.abc import Sequence\n",
    "\n",
    "from unetr import CustomedUNETR\n",
    "\n",
    "import json\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import EnsureTyped\n",
    "from monai.transforms import Compose, LoadImaged, ScaleIntensityRanged, ConcatItemsd\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceCELoss, DiceFocalLoss, FocalLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "\n",
    "from monai.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    MapTransform,\n",
    "    ScaleIntensityd,\n",
    "    #AddChanneld,\n",
    "    SpatialPadd,\n",
    "    CenterSpatialCropd,\n",
    "    EnsureChannelFirstd,\n",
    "    ConcatItemsd,\n",
    "    AdjustContrastd, \n",
    "    Rand3DElasticd,\n",
    "    HistogramNormalized,\n",
    "    NormalizeIntensityd,\n",
    "    Invertd,\n",
    "    SaveImage,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "\n",
    "import torch\n",
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped'\n",
    "json_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped/MDA_ct_train_new.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datafold_read(datalist, basedir, fold=0, key=\"training\"):\n",
    "    with open(datalist) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    json_data = json_data[key]\n",
    "\n",
    "    for d in json_data:\n",
    "        for k in d:\n",
    "            if isinstance(d[k], list):\n",
    "                d[k] = [os.path.join(basedir, iv) for iv in d[k]]\n",
    "            elif isinstance(d[k], str):\n",
    "                d[k] = os.path.join(basedir, d[k]) if len(d[k]) > 0 else d[k]\n",
    "\n",
    "    tr = []\n",
    "    val = []\n",
    "    for d in json_data:\n",
    "        if \"fold\" in d and d[\"fold\"] == fold:\n",
    "            val.append(d)\n",
    "        else:\n",
    "            tr.append(d)\n",
    "\n",
    "    return tr, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 44)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files, validation_files = datafold_read(datalist=json_dir, basedir=data_dir, fold=0)\n",
    "len(train_files), len(validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipCT(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on hecktor classes:\n",
    "    label 1 is the tumor\n",
    "    label 2 is the lymph node\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            if key == \"ct\":\n",
    "                d[key] = torch.clip(d[key], min=-200, max=200)\n",
    "            # elif key == \"pt\":\n",
    "            #     d[key] = torch.clip(d[key], d[key].min(), 5)\n",
    "        return d\n",
    "\n",
    "class MulPTFM(MapTransform):\n",
    "    \"\"\"\n",
    "    Mult PT and FM \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "\n",
    "        fm = d[\"ct\"] > 0\n",
    "        d[\"pt\"] = d[\"pt\"] * fm\n",
    "        return d\n",
    "\n",
    "class SelectClass(MapTransform):\n",
    "    \"\"\"\n",
    "    Select the class for which you want to fine tune the model \n",
    "\n",
    "    \"\"\"\n",
    "    # def __init__(self, keys, cls=1):\n",
    "    #     super(self).__init__(keys)\n",
    "    #     self.cls = cls\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        d[\"seg\"][d[\"seg\"] == 1] = 0\n",
    "        # d[\"seg\"][d[\"seg\"] == 2] = 1\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"ct\", \"seg\"], ensure_channel_first = True),\n",
    "        SpatialPadd(keys=[\"ct\",  \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "        Orientationd(keys=[\"ct\",  \"seg\"], axcodes=\"PLS\"),\n",
    "        #NormalizeIntensityd(keys=[\"pt\"]),\n",
    "        ClipCT(keys=[\"ct\"]),\n",
    "        ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "        #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "        #ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),\n",
    "        #NormalizeIntensityd(keys=[\"ctpt\"], channel_wise=True),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"ct\", \"seg\"],\n",
    "            label_key=\"seg\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=num_samples,\n",
    "            image_key=\"ct\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"ct\", \"seg\"],\n",
    "            spatial_axis=[0],\n",
    "            prob=0.20,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"ct\", \"seg\"],\n",
    "            spatial_axis=[1],\n",
    "            prob=0.20,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"ct\", \"seg\"],\n",
    "            spatial_axis=[2],\n",
    "            prob=0.20,\n",
    "        ),\n",
    "        RandRotate90d(\n",
    "            keys=[\"ct\", \"seg\"],\n",
    "            prob=0.20,\n",
    "            max_k=3,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"ct\", \"seg\"], ensure_channel_first = True),\n",
    "        SpatialPadd(keys=[\"ct\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "        Orientationd(keys=[\"ct\",  \"seg\"], axcodes=\"PLS\"),\n",
    "        #NormalizeIntensityd(keys=[\"pt\"]),\n",
    "        ClipCT(keys=[\"ct\"]),\n",
    "        ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "        #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "        #ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "val_ds = monai.data.Dataset(data=validation_files, transform=val_transforms)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=2, \n",
    "    num_workers=8, \n",
    "    shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: unetr CustomedUNETR.__init__:pos_embed: Argument `pos_embed` has been deprecated since version 1.2. It will be removed in version 1.4. please use `proj_type` instead.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nada's version of UNETR\n",
      "(96, 96, 96)\n",
      "(16, 16, 16)\n",
      "(6, 6, 6)\n",
      "768\n",
      "zaz w dakchi ya s lbnat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomedUNETR(\n",
    "    in_channels=1,  # Number of input channels\n",
    "    out_channels=3,  # Number of output channels\n",
    "    img_size=(96, 96, 96),  # Size of the input image\n",
    "    feature_size=48,  # Size of the feature maps\n",
    "    hidden_size=768,\n",
    "    num_heads=12,  # Size of the hidden layers in the transformer\n",
    "    mlp_dim=3072,  # Dimension of the MLP in the transformer\n",
    "    pos_embed=\"perceptron\",  # Type of positional embedding\n",
    "    norm_name=\"instance\",  # Type of normalization\n",
    "    res_block=True,  # Whether to use residual blocks\n",
    "    dropout_rate=0.0,\n",
    "    proj_type=\"conv\",\n",
    "    r=4,\n",
    "    lora_layer=False,\n",
    "    use_ct_encoder=True,\n",
    "    use_pet_encoder=False,\n",
    ").to(device)\n",
    "\n",
    "pt_weights_4 = '/home/nada.saadi/CTPET/hecktor2022_cropped/Module1-4centers-CT-only-tokens/-module1-4centers-ctonly-tokens.pth'\n",
    "\n",
    "state_dict = torch.load(pt_weights_4)\n",
    "model_state_dict = model.state_dict()\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "#add the code for freezing the layers of the decoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "def poly_lr(epoch, max_epochs, initial_lr, exponent=0.9):\n",
    "    return initial_lr * (1 - epoch / max_epochs)**exponent\n",
    "\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = ''\n",
    "def validation(epoch_iterator_val):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(epoch_iterator_val):\n",
    "                val_inputs, val_labels = (batch[\"ct\"].cuda(), batch[\"seg\"].cuda())\n",
    "                val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, new_model)\n",
    "                val_labels_list = decollate_batch(val_labels)\n",
    "                val_labels_convert = [\n",
    "                    post_label(val_label_tensor) for val_label_tensor in val_labels_list\n",
    "                ]\n",
    "                val_outputs_list = decollate_batch(val_outputs)\n",
    "                val_output_convert = [\n",
    "                    post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list\n",
    "                ]\n",
    "                dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "                dice_metric_batch(y_pred=val_output_convert, y=val_labels_convert)\n",
    "                epoch_iterator_val.set_description(\n",
    "                    \"Validate (%d / %d Steps)\" % (global_step, 10.0)\n",
    "                )\n",
    "            mean_dice_val = dice_metric.aggregate().item()\n",
    "            metric_batch_val = dice_metric_batch.aggregate()\n",
    "\n",
    "            metric_tumor = metric_batch_val[0].item()\n",
    "            metric_lymph = metric_batch_val[1].item()\n",
    "\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "        return mean_dice_val, metric_tumor, metric_lymph\n",
    "\n",
    "\n",
    "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        epoch_iterator = tqdm(\n",
    "            train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True\n",
    "        )\n",
    "        \n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            step += 1\n",
    "            x, y = (batch[\"ct\"].cuda(), batch[\"seg\"].cuda())\n",
    "            logit_map = model(x)\n",
    "            loss = loss_function(logit_map, y)\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_iterator.set_description(\n",
    "                \"Training (%d / %d Steps) (loss=%2.5f)\"\n",
    "                % (global_step, max_iterations, loss)\n",
    "            )\n",
    "            \n",
    "            if (\n",
    "                global_step % eval_num == 0 and global_step != 0\n",
    "            ) or global_step == max_iterations:\n",
    "                epoch_iterator_val = tqdm(\n",
    "                    val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True\n",
    "                )\n",
    "                dice_val, metric_tumor, metric_lymph = validation(epoch_iterator_val)\n",
    "                epoch_loss /= step\n",
    "                epoch_loss_values.append(epoch_loss)\n",
    "                metric_values.append(dice_val)\n",
    "                metric_values_tumor.append(metric_tumor)\n",
    "                metric_values_lymph.append(metric_lymph)\n",
    "                if dice_val > dice_val_best:\n",
    "                    dice_val_best = dice_val\n",
    "                    global_step_best = global_step\n",
    "                    torch.save(\n",
    "                        model.state_dict(), os.path.join(model_dir, \"5thcenterctonlypret.pth\")\n",
    "                    )\n",
    "                    print(\n",
    "                        \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {} Current Avg. tumor Dice: {} Current Avg. lymph Dice: {}\".format(\n",
    "                            dice_val_best, dice_val, metric_tumor, metric_lymph\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {} Current Avg. tumor Dice: {} Current Avg. lymph Dice: {}\".format(\n",
    "                            dice_val_best, dice_val,  metric_tumor, metric_lymph\n",
    "                        )\n",
    "                    )\n",
    "            global_step += 1\n",
    "        return global_step, dice_val_best, global_step_best\n",
    "\n",
    "       \n",
    "\n",
    "max_iterations = 18000\n",
    "eval_num = 100\n",
    "\n",
    "post_label = AsDiscrete(to_onehot=3)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=3)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "dice_metric_batch = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
    "\n",
    "epoch = 0\n",
    "max_num_epochs = 530\n",
    "\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tumor = []\n",
    "metric_values_lymph = []\n",
    "while global_step < max_iterations:\n",
    "    global_step, dice_val_best, global_step_best = train(\n",
    "        global_step, train_loader, dice_val_best, global_step_best\n",
    "    )\n",
    "    # wandb.log({'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "    # wandb.log({'Best Dice': dice_val_best})\n",
    "    epoch += 1 \n",
    "    #optimizer.param_groups[0]['lr'] = poly_lr(epoch, max_num_epochs, 0.005676 , 0.9)\n",
    "# model.load_state_dict(torch.load(os.path.join(model_dir, \"best_metric_luck_UNETr_prompt.pth\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
