{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock, UnetrPrUpBlock, UnetrUpBlock\n",
    "from monai.networks.nets.vit import ViT\n",
    "from monai.utils import deprecated_arg, ensure_tuple_rep\n",
    "\n",
    "\n",
    "class UNETR(nn.Module):\n",
    "    \"\"\"\n",
    "    UNETR based on: \"Hatamizadeh et al.,\n",
    "    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>\"\n",
    "    \"\"\"\n",
    "\n",
    "    @deprecated_arg(\n",
    "        name=\"pos_embed\", since=\"1.2\", removed=\"1.4\", new_name=\"proj_type\", msg_suffix=\"please use `proj_type` instead.\"\n",
    "    )\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        img_size: Sequence[int] | int,\n",
    "        feature_size: int = 16,\n",
    "        hidden_size: int = 768,\n",
    "        mlp_dim: int = 3072,\n",
    "        num_heads: int = 12,\n",
    "        pos_embed: str = \"conv\",\n",
    "        proj_type: str = \"conv\",\n",
    "        norm_name: tuple | str = \"instance\",\n",
    "        conv_block: bool = True,\n",
    "        res_block: bool = True,\n",
    "        dropout_rate: float = 0.0,\n",
    "        spatial_dims: int = 3,\n",
    "        qkv_bias: bool = False,\n",
    "        save_attn: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: dimension of input channels.\n",
    "            out_channels: dimension of output channels.\n",
    "            img_size: dimension of input image.\n",
    "            feature_size: dimension of network feature size. Defaults to 16.\n",
    "            hidden_size: dimension of hidden layer. Defaults to 768.\n",
    "            mlp_dim: dimension of feedforward layer. Defaults to 3072.\n",
    "            num_heads: number of attention heads. Defaults to 12.\n",
    "            proj_type: patch embedding layer type. Defaults to \"conv\".\n",
    "            norm_name: feature normalization type and arguments. Defaults to \"instance\".\n",
    "            conv_block: if convolutional block is used. Defaults to True.\n",
    "            res_block: if residual block is used. Defaults to True.\n",
    "            dropout_rate: fraction of the input units to drop. Defaults to 0.0.\n",
    "            spatial_dims: number of spatial dims. Defaults to 3.\n",
    "            qkv_bias: apply the bias term for the qkv linear layer in self attention block. Defaults to False.\n",
    "            save_attn: to make accessible the attention in self attention block. Defaults to False.\n",
    "\n",
    "        .. deprecated:: 1.4\n",
    "            ``pos_embed`` is deprecated in favor of ``proj_type``.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # for single channel input 4-channel output with image size of (96,96,96), feature size of 32 and batch norm\n",
    "            >>> net = UNETR(in_channels=1, out_channels=4, img_size=(96,96,96), feature_size=32, norm_name='batch')\n",
    "\n",
    "             # for single channel input 4-channel output with image size of (96,96), feature size of 32 and batch norm\n",
    "            >>> net = UNETR(in_channels=1, out_channels=4, img_size=96, feature_size=32, norm_name='batch', spatial_dims=2)\n",
    "\n",
    "            # for 4-channel input 3-channel output with image size of (128,128,128), conv position embedding and instance norm\n",
    "            >>> net = UNETR(in_channels=4, out_channels=3, img_size=(128,128,128), proj_type='conv', norm_name='instance')\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not (0 <= dropout_rate <= 1):\n",
    "            raise ValueError(\"dropout_rate should be between 0 and 1.\")\n",
    "\n",
    "        if hidden_size % num_heads != 0:\n",
    "            raise ValueError(\"hidden_size should be divisible by num_heads.\")\n",
    "\n",
    "        self.num_layers = 12\n",
    "        img_size = ensure_tuple_rep(img_size, spatial_dims)\n",
    "        self.patch_size = ensure_tuple_rep(16, spatial_dims)\n",
    "        self.feat_size = tuple(img_d // p_d for img_d, p_d in zip(img_size, self.patch_size))\n",
    "        self.hidden_size = hidden_size\n",
    "        self.classification = False\n",
    "        self.vit = ViT(\n",
    "            in_channels=in_channels,\n",
    "            img_size=img_size,\n",
    "            patch_size=self.patch_size,\n",
    "            hidden_size=hidden_size,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=num_heads,\n",
    "            proj_type=proj_type,\n",
    "            classification=self.classification,\n",
    "            dropout_rate=dropout_rate,\n",
    "            spatial_dims=spatial_dims,\n",
    "            qkv_bias=qkv_bias,\n",
    "            save_attn=save_attn,\n",
    "        )\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder2 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 2,\n",
    "            num_layer=2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder3 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 4,\n",
    "            num_layer=1,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder4 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "            num_layer=0,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 8,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n",
    "        self.proj_axes = (0, spatial_dims + 1) + tuple(d + 1 for d in range(spatial_dims))\n",
    "        self.proj_view_shape = list(self.feat_size) + [self.hidden_size]\n",
    "\n",
    "    def proj_feat(self, x):\n",
    "        new_view = [x.size(0)] + self.proj_view_shape\n",
    "        x = x.view(new_view)\n",
    "        x = x.permute(self.proj_axes).contiguous()\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \n",
    "        x, hidden_states_out = self.vit(x_in)\n",
    "        #2N--> N\n",
    "        #x = x[:,0,:,:,:]  \n",
    "        enc1 = self.encoder1(x_in)\n",
    "      \n",
    "        x2 = hidden_states_out[3]\n",
    "        x2 = x2[:, ::2, :, :, :]  # Divide size by half along the second dimension\n",
    "        \n",
    "        enc2 = self.encoder2(self.proj_feat(x2))\n",
    "        x3 = hidden_states_out[6]\n",
    "        x3 = x3[:, ::2, :, :, :]  \n",
    "        \n",
    "        enc3 = self.encoder3(self.proj_feat(x3))\n",
    "        x4 = hidden_states_out[9]\n",
    "        x4 = x4[:, ::2, :, :, :] \n",
    "        \n",
    "        enc4 = self.encoder4(self.proj_feat(x4))\n",
    "        \n",
    "        dec4 = self.proj_feat(x)\n",
    "        \n",
    "        dec3 = self.decoder5(dec4, enc4)\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        out = self.decoder2(dec1, enc1)\n",
    "        return self.out(out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
