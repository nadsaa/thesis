{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "import torch\n",
    "torch.cuda.device_count()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from collections.abc import Sequence\n",
    "\n",
    "from unetr import CustomedUNETR\n",
    "\n",
    "import json\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "# import wandb\n",
    "\n",
    "import monai\n",
    "from monai.losses import DiceCELoss, DiceFocalLoss, FocalLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "\n",
    "from monai.transforms import (\n",
    "       AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    MapTransform,\n",
    "    ScaleIntensityd,\n",
    "    #AddChanneld,\n",
    "    SpatialPadd,\n",
    "    CenterSpatialCropd,\n",
    "    EnsureChannelFirstd,\n",
    "    ConcatItemsd,\n",
    "    AdjustContrastd, \n",
    "    Rand3DElasticd,\n",
    "    HistogramNormalized,\n",
    "    NormalizeIntensityd,\n",
    "    Invertd,\n",
    "    SaveImage,\n",
    "\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR, UNETR, SegResNet\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "from monai import data\n",
    "\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "\n",
    "import torch\n",
    "from monai.transforms import apply_transform\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import AsDiscrete\n",
    "from monai.data import DataLoader, Dataset, decollate_batch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped'\n",
    "# #json_dir = '/home/nada.saadi/CTPET/hecktor2022_cropped/HGJ_CT_train_new.json'\n",
    "# json_dirr = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_modified_validation_files.json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped'\n",
    "#json_dir = '/home/nada.saadi/CTPET/hecktor2022_cropped/HGJ_CT_train_new.json'\n",
    "json_dir ='/home/nada.saadi/CTPET/hecktor2022_cropped/HGJ_PET_train_new.json'\n",
    "#json_dir='/home/nada.saadi/CTPET/hecktor2022_cropped/HGJ_ctpt_train_new.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datafold_read(datalist, basedir, fold=0,key=\"training\"):\n",
    "    with open(datalist) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    json_data = json_data[key]\n",
    "\n",
    "    for d in json_data:\n",
    "        for k in d:\n",
    "            if isinstance(d[k], list):\n",
    "                d[k] = [os.path.join(basedir, iv) for iv in d[k]]\n",
    "            elif isinstance(d[k], str):\n",
    "                d[k] = os.path.join(basedir, d[k]) if len(d[k]) > 0 else d[k]\n",
    "\n",
    "    tr = []\n",
    "    val = []\n",
    "    for d in json_data:\n",
    "        if \"fold\" in d and d[\"fold\"] == fold:\n",
    "            val.append(d)\n",
    "        else:\n",
    "            tr.append(d)\n",
    "\n",
    "    return tr, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 15)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "train_files, validation_files = datafold_read(datalist=json_dir, basedir=data_dir, fold=0)\n",
    "len(train_files), len(validation_files)\n",
    "# Path for the new JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation files have been saved to /home/nada.saadi/CTPET/hecktor2022_cropped/HGJ_PET_train_new\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "train_files, validation_files = datafold_read(datalist=json_dir, basedir=data_dir, fold=0)\n",
    "#len(train_files), len(validation_files)\n",
    "# Path for the new JSON file\n",
    "#train_files, validation_files = datafold_read(datalist=json_dir_7th, basedir=data_dir, fold=0)\n",
    "#ct_validation_json_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_ct_train_new.json'\n",
    "\n",
    "pt_validation_json_path='/home/nada.saadi/CTPET/hecktor2022_cropped/HGJ_PET_train_new'\n",
    "#ctpt_validation_json_path='/home/nada.saadi/CTPET/hecktor2022_cropped/HGJ_ctpt_train_new.json'\n",
    "# Save the validation files to a JSON file\n",
    "with open(pt_validation_json_path, 'w') as file:\n",
    "    json.dump(validation_files, file, indent=4)\n",
    "\n",
    "print(f\"Validation files have been saved to {pt_validation_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_files)\n",
    "#hgj_ct_validation_files = '/home/nada.saadi/CTPET/hecktor2022_cropped/hgj_ct_validation_files.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipCT(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on hecktor classes:\n",
    "    label 1 is the tumor\n",
    "    label 2 is the lymph node\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            if key == \"ct\":\n",
    "                d[key] = torch.clip(d[key], min=-200, max=200)\n",
    "            # elif key == \"pt\":\n",
    "            #     d[key] = torch.clip(d[key], d[key].min(), 5)\n",
    "        return d\n",
    "\n",
    "class MulPTFM(MapTransform):\n",
    "    \"\"\"\n",
    "    Mult PT and FM \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "\n",
    "        fm = d[\"ct\"] > 0\n",
    "        d[\"pt\"] = d[\"pt\"] * fm\n",
    "        return d\n",
    "\n",
    "class SelectClass(MapTransform):\n",
    "    \"\"\"\n",
    "    Select the class for which you want to fine tune the model \n",
    "\n",
    "    \"\"\"\n",
    "    # def __init__(self, keys, cls=1):\n",
    "    #     super(self).__init__(keys)\n",
    "    #     self.cls = cls\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        d[\"seg\"][d[\"seg\"] == 1] = 0\n",
    "        # d[\"seg\"][d[\"seg\"] == 2] = 1\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = 4\n",
    "\n",
    "# train_transforms = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"ct\", \"pt\", \"seg\"], ensure_channel_first = True),\n",
    "#         SpatialPadd(keys=[\"ct\", \"pt\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "#         Orientationd(keys=[\"ct\", \"pt\", \"seg\"], axcodes=\"PLS\"),\n",
    "#         NormalizeIntensityd(keys=[\"pt\"]),\n",
    "#         ClipCT(keys=[\"ct\"]),\n",
    "#         ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "#         #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "#         ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),\n",
    "#         #NormalizeIntensityd(keys=[\"ctpt\"], channel_wise=True),\n",
    "#         RandCropByPosNegLabeld(\n",
    "#             keys=[\"ctpt\", \"seg\"],\n",
    "#             label_key=\"seg\",\n",
    "#             spatial_size=(96, 96, 96),\n",
    "#             pos=1,\n",
    "#             neg=1,\n",
    "#             num_samples=num_samples,\n",
    "#             image_key=\"ctpt\",\n",
    "#             image_threshold=0,\n",
    "#         ),\n",
    "#         RandFlipd(\n",
    "#             keys=[\"ctpt\", \"seg\"],\n",
    "#             spatial_axis=[0],\n",
    "#             prob=0.20,\n",
    "#         ),\n",
    "#         RandFlipd(\n",
    "#             keys=[\"ctpt\", \"seg\"],\n",
    "#             spatial_axis=[1],\n",
    "#             prob=0.20,\n",
    "#         ),\n",
    "#         RandFlipd(\n",
    "#             keys=[\"ctpt\", \"seg\"],\n",
    "#             spatial_axis=[2],\n",
    "#             prob=0.20,\n",
    "#         ),\n",
    "#         RandRotate90d(\n",
    "#             keys=[\"ctpt\", \"seg\"],\n",
    "#             prob=0.20,\n",
    "#             max_k=3,\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "# val_transforms_ctpt = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"ct\", \"pt\", \"seg\"], ensure_channel_first = True),\n",
    "#         SpatialPadd(keys=[\"ct\", \"pt\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "#         Orientationd(keys=[\"ct\", \"pt\", \"seg\"], axcodes=\"PLS\"),\n",
    "#         NormalizeIntensityd(keys=[\"pt\"]),\n",
    "#         ClipCT(keys=[\"ct\"]),\n",
    "#         ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "#         #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "#         ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"pt\", \"seg\"], ensure_channel_first = True),\n",
    "        SpatialPadd(keys=[\"pt\",  \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "        Orientationd(keys=[\"pt\",  \"seg\"], axcodes=\"PLS\"),\n",
    "        #NormalizeIntensityd(keys=[\"pt\"]),\n",
    "        ClipCT(keys=[\"pt\"]),\n",
    "        ScaleIntensityd(keys=[\"pt\"], minv=0, maxv=1),\n",
    "        #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "        #ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),\n",
    "        #NormalizeIntensityd(keys=[\"ctpt\"], channel_wise=True),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"pt\", \"seg\"],\n",
    "            label_key=\"seg\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=num_samples,\n",
    "            image_key=\"pt\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"pt\", \"seg\"],\n",
    "            spatial_axis=[0],\n",
    "            prob=0.20,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"pt\", \"seg\"],\n",
    "            spatial_axis=[1],\n",
    "            prob=0.20,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"pt\", \"seg\"],\n",
    "            spatial_axis=[2],\n",
    "            prob=0.20,\n",
    "        ),\n",
    "        RandRotate90d(\n",
    "            keys=[\"pt\", \"seg\"],\n",
    "            prob=0.20,\n",
    "            max_k=3,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "val_transforms_pt = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"pt\", \"seg\"], ensure_channel_first = True),\n",
    "        SpatialPadd(keys=[\"pt\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "        Orientationd(keys=[\"pt\",  \"seg\"], axcodes=\"PLS\"),\n",
    "        #NormalizeIntensityd(keys=[\"pt\"]),\n",
    "        ClipCT(keys=[\"pt\"]),\n",
    "        ScaleIntensityd(keys=[\"pt\"], minv=0, maxv=1),\n",
    "        #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "        #ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = 4\n",
    "\n",
    "# train_transforms = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"ct\", \"seg\"], ensure_channel_first = True),\n",
    "#         SpatialPadd(keys=[\"ct\",  \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "#         Orientationd(keys=[\"ct\",  \"seg\"], axcodes=\"PLS\"),\n",
    "#         #NormalizeIntensityd(keys=[\"pt\"]),\n",
    "#         ClipCT(keys=[\"ct\"]),\n",
    "#         ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "#         #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "#         #ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),\n",
    "#         #NormalizeIntensityd(keys=[\"ctpt\"], channel_wise=True),\n",
    "#         RandCropByPosNegLabeld(\n",
    "#             keys=[\"ct\", \"seg\"],\n",
    "#             label_key=\"seg\",\n",
    "#             spatial_size=(96, 96, 96),\n",
    "#             pos=1,\n",
    "#             neg=1,\n",
    "#             num_samples=num_samples,\n",
    "#             image_key=\"ct\",\n",
    "#             image_threshold=0,\n",
    "#         ),\n",
    "#         RandFlipd(\n",
    "#             keys=[\"ct\", \"seg\"],\n",
    "#             spatial_axis=[0],\n",
    "#             prob=0.20,\n",
    "#         ),\n",
    "#         RandFlipd(\n",
    "#             keys=[\"ct\", \"seg\"],\n",
    "#             spatial_axis=[1],\n",
    "#             prob=0.20,\n",
    "#         ),\n",
    "#         RandFlipd(\n",
    "#             keys=[\"ct\", \"seg\"],\n",
    "#             spatial_axis=[2],\n",
    "#             prob=0.20,\n",
    "#         ),\n",
    "#         RandRotate90d(\n",
    "#             keys=[\"ct\", \"seg\"],\n",
    "#             prob=0.20,\n",
    "#             max_k=3,\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "# val_transforms = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"ct\", \"seg\"], ensure_channel_first = True),\n",
    "#         SpatialPadd(keys=[\"ct\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "#         Orientationd(keys=[\"ct\",  \"seg\"], axcodes=\"PLS\"),\n",
    "#         #NormalizeIntensityd(keys=[\"pt\"]),\n",
    "#         ClipCT(keys=[\"ct\"]),\n",
    "#         ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "#         #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "#         #ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "val_ds = monai.data.Dataset(data=validation_files, transform=val_transforms_ctpt)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=2, \n",
    "    num_workers=8, \n",
    "    shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.unetr UNETR.__init__:pos_embed: Argument `pos_embed` has been deprecated since version 1.2. It will be removed in version 1.4. please use `proj_type` instead.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: unetr CustomedUNETR.__init__:pos_embed: Argument `pos_embed` has been deprecated since version 1.2. It will be removed in version 1.4. please use `proj_type` instead.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nada's version of UNETR\n",
      "(96, 96, 96)\n",
      "(16, 16, 16)\n",
      "(6, 6, 6)\n",
      "768\n",
      "zaz w dakchi ya s lbnat\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    img_size=(96, 96, 96),\n",
    "    feature_size=16, #16\n",
    "    hidden_size= 768, #768,\n",
    "    mlp_dim=3072, #3072,\n",
    "    num_heads=12,\n",
    "    pos_embed=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "model_our = CustomedUNETR(\n",
    "    in_channels=1,  # Number of input channels\n",
    "    out_channels=3,  # Number of output channels\n",
    "    img_size=(96, 96, 96),  # Size of the input image\n",
    "    feature_size=48,  # Size of the feature maps\n",
    "    hidden_size=768,\n",
    "    num_heads = 12,# Size of the hidden layers in the transformer\n",
    "    mlp_dim=3072,  # Dimension of the MLP in the transformer\n",
    "    pos_embed=\"perceptron\",  # Type of positional embedding\n",
    "    norm_name=\"instance\",  # Type of normalization\n",
    "    res_block=True,  # Whether to use residual blocks\n",
    "    dropout_rate=0.0,\n",
    "    proj_type= \"conv\",\n",
    "    r=4,\n",
    "    lora_layer=True,\n",
    "    use_ct_encoder=True,\n",
    "    use_pet_encoder=False    \n",
    ").to(device) \n",
    "# pt_weights_ct_only_6th='/home/nada.saadi/CTPET/hecktor2022_cropped/6th_CT_ourapproach/6th_center_CT_our_approach.pth'\n",
    "# state_dict = torch.load(pt_weights_ct_only_6th)\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"decoder5\" in name or \"decoder4\" in name or \"decoder3\" in name or \"decoder2\" in name:\n",
    "#         param.requires_grad = False\n",
    "# PT_6TH_CHANNEL_CT='/home/nada.saadi/CTPET/hecktor2022_cropped/6thCTonly_channel/6thCTonly_channel.pth'\n",
    "# torch.load(PT_6TH_CHANNEL_CT)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"decoder5\" in name or \"decoder4\" in name or \"decoder3\" in name or \"decoder2\" in name:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_transforms_ctpt = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"ct\", \"pt\", \"seg\"], ensure_channel_first = True),\n",
    "#         SpatialPadd(keys=[\"ct\", \"pt\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "#         Orientationd(keys=[\"ct\", \"pt\", \"seg\"], axcodes=\"PLS\"),\n",
    "#         NormalizeIntensityd(keys=[\"pt\"]),\n",
    "#         ClipCT(keys=[\"ct\"]),\n",
    "#         ScaleIntensityd(keys=[\"ct\"], minv=0, maxv=1),\n",
    "#         #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "#         ConcatItemsd(keys=[\"pt\", \"ct\"], name=\"ctpt\"),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15\n",
      "Validation files have been saved to /home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_validation_files.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "train_files, validation_files = datafold_read(datalist=json_dir, basedir=data_dir, fold=0)\n",
    "print(len(train_files), len(validation_files))\n",
    "# Path for the new JSON file\n",
    "ct_validation_json_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_validation_files.json'\n",
    "\n",
    "# Save the validation files to a JSON file\n",
    "with open(ct_validation_json_path, 'w') as file:\n",
    "    json.dump(validation_files, file, indent=4)\n",
    "\n",
    "print(f\"Validation files have been saved to {ct_validation_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "with open(pt_validation_json_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=Dataset(data=test_data, transform=val_transforms_pt),\n",
    "    batch_size=1,  # Batch size for testing can be 1 since no backpropagation is required\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 6, 6, 6, 768]' is invalid for input of size 331776",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_dice_test, metric_tumor, metric_lymph\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Save the weights\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[43mtesting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 46\u001b[0m, in \u001b[0;36mtesting\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader):\n\u001b[1;32m     44\u001b[0m     test_inputs, test_labels \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda(), batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 46\u001b[0m     test_outputs_pt \u001b[38;5;241m=\u001b[39m \u001b[43msliding_window_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_our\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Convert outputs and labels to one-hot format for DiceMetric calculation\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     test_outputs_ct \u001b[38;5;241m=\u001b[39m [AsDiscrete(argmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, to_onehot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m decollate_batch(test_outputs_pt)]\n",
      "File \u001b[0;32m~/miniconda3/envs/clam/lib/python3.8/site-packages/monai/inferers/utils.py:229\u001b[0m, in \u001b[0;36msliding_window_inference\u001b[0;34m(inputs, roi_size, sw_batch_size, predictor, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, roi_weight_map, process_fn, buffer_steps, buffer_dim, with_coord, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     seg_prob_out \u001b[38;5;241m=\u001b[39m predictor(win_data, unravel_slice, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# batched patch\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     seg_prob_out \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwin_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# batched patch\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# convert seg_prob_out to tuple seg_tuple, this does not allocate new memory.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m dict_keys, seg_tuple \u001b[38;5;241m=\u001b[39m _flatten_struct(seg_prob_out)\n",
      "File \u001b[0;32m~/miniconda3/envs/clam/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/clam/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/CTPET/hecktor2022_cropped/unetr.py:411\u001b[0m, in \u001b[0;36mCustomedUNETR.forward\u001b[0;34m(self, x_in, mode)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m#print(f\"After slicing: {x2.shape}\")\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m x2 \u001b[38;5;66;03m# No slicing needed\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m enc2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    413\u001b[0m x3 \u001b[38;5;241m=\u001b[39m hidden_states_out[\u001b[38;5;241m6\u001b[39m]\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mct\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/CTPET/hecktor2022_cropped/unetr.py:313\u001b[0m, in \u001b[0;36mCustomedUNETR.proj_feat\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    309\u001b[0m new_view \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m+\u001b[39m proj_view_shape\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# In proj_feat:\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# print(f\"Before reshaping x : {x.shape}\")\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_view\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# print(f\"After reshaping x: {x.shape}\")\u001b[39;00m\n\u001b[1;32m    315\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_axes)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 6, 6, 6, 768]' is invalid for input of size 331776"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# with open(json_dir, 'r') as f:\n",
    "#     test_data = json.load(f)\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     dataset=Dataset(data=test_data, transform=val_transforms),\n",
    "#     batch_size=1,  # Batch size for testing can be 1 since no backpropagation is required\n",
    "#     shuffle=False,\n",
    "#     num_workers=4\n",
    "# )\n",
    "\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.AdamW(model_our.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "post_label = AsDiscrete(to_onehot=3)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=3)\n",
    "\n",
    "# Dice metric for evaluation\n",
    "dice_metric_fn = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "dice_metric_batch_fn = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
    "\n",
    "rt='/home/nada.saadi/CTPET/hecktor2022_cropped/a_6th/a_6th_ctptct.pth'\n",
    "model_our.load_state_dict(torch.load(rt), strict=False)\n",
    "model_state_dict = model.state_dict()\n",
    "state_dict = {}  # Define the \"state_dict\" variable\n",
    "for name, param in state_dict.items():\n",
    "    if name in model_state_dict:\n",
    "        if param.shape == model_state_dict[name].shape:\n",
    "            model_state_dict[name] = param\n",
    "#model_our.encoder1_pt.load_state_dict(model_our.encoder1.state_dict(), strict=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def testing():\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    dice_metric_fn.reset()\n",
    "    dice_metric_batch_fn.reset()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(test_loader):\n",
    "            test_inputs, test_labels = batch_data[\"pt\"].cuda(), batch_data[\"seg\"].cuda()\n",
    "            \n",
    "            test_outputs_pt = sliding_window_inference(test_inputs, (96, 96, 96), 4, model_our)\n",
    "            \n",
    "            # Convert outputs and labels to one-hot format for DiceMetric calculation\n",
    "            test_outputs_ct = [AsDiscrete(argmax=True, to_onehot=3)(i) for i in decollate_batch(test_outputs_pt)]\n",
    "            test_labels = [AsDiscrete(to_onehot=3)(i) for i in decollate_batch(test_labels)]\n",
    "\n",
    "            dice_metric_fn(y_pred=test_outputs_ctpt, y=test_labels)\n",
    "            dice_metric_batch_fn(y_pred=test_outputs_pt, y=test_labels)\n",
    "            \n",
    "        # After processing all batches, save the outputs\n",
    "    #ct_save_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/test_output_ct_saved.pt'\n",
    "    # ct_save_path_7thcenter = '/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_test_output_ct_saved.pt'\n",
    "    # torch.save(test_outputs_ct, ct_save_path_7thcenter)\n",
    "    # print(f\"Outputs saved at: {ct_save_path_7thcenter}\")\n",
    "    \n",
    "\n",
    "    mean_dice_test = dice_metric_fn.aggregate().item()\n",
    "    metric_batch_test = dice_metric_batch_fn.aggregate()\n",
    "    metric_tumor = metric_batch_test[0].item()\n",
    "    metric_lymph = metric_batch_test[1].item()\n",
    "\n",
    "    print(f\"Testing - Avg Dice: {mean_dice_test:.4f}, Tumor Dice: {metric_tumor:.4f}, Lymph Dice: {metric_lymph:.4f}\")\n",
    "    return mean_dice_test, metric_tumor, metric_lymph\n",
    "    # Save the weights\n",
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['layer.conv1.conv.weight', 'layer.conv2.conv.weight', 'layer.conv3.conv.weight'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_our.encoder1.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['layer.conv1.conv.weight', 'layer.conv2.conv.weight', 'layer.conv3.conv.weight'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_our.encoder1_pt.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_our.encoder1_pt.load_state_dict(model_our.encoder1.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_state_dict = model.state_dict()\n",
    "# state_dict = {}  # Define the \"state_dict\" variable\n",
    "# for name, param in state_dict.items():\n",
    "#     if name in model_state_dict:\n",
    "#         if param.shape == model_state_dict[name].shape:\n",
    "#             model_state_dict[name] = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state dict from file\n",
    "loaded_state_dict = torch.load(rt)\n",
    "\n",
    "# Get the model's state dict\n",
    "model_state_dict = model_our.state_dict()\n",
    "\n",
    "# Filter out loaded state dict keys that do not match the model's state dict\n",
    "filtered_state_dict = {k: v for k, v in loaded_state_dict.items() if k in model_state_dict and model_state_dict[k].size() == v.size()}\n",
    "\n",
    "# Update the model's state dict with the filtered state dict\n",
    "model_our.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "# Optionally, identify and log the keys that were ignored\n",
    "ignored_keys = [k for k in loaded_state_dict if k not in filtered_state_dict]\n",
    "if ignored_keys:\n",
    "    print(\"The following keys were ignored due to mismatch:\", ignored_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
