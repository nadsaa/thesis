{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_model_7='/home/nada.saadi/CTPET/hecktor2022_cropped/7thctchannel/7thCTonly_channel.pth'\n",
    "pt_model_7='/home/nada.saadi/CTPET/hecktor2022_cropped/7thPETchannelonly/7thPETonly_channel.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "# import wandb\n",
    "\n",
    "import monai\n",
    "from monai.losses import DiceCELoss, DiceFocalLoss, FocalLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    MapTransform,\n",
    "    ScaleIntensityd,\n",
    "    #AddChanneld,\n",
    "    SpatialPadd,\n",
    "    CenterSpatialCropd,\n",
    "    EnsureChannelFirstd,\n",
    "    ConcatItemsd,\n",
    "    AdjustContrastd, \n",
    "    Rand3DElasticd,\n",
    "    HistogramNormalized,\n",
    "    NormalizeIntensityd,\n",
    "    Invertd,\n",
    "    SaveImage,\n",
    "\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR, UNETR, SegResNet\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "from monai import data\n",
    "\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "\n",
    "from monai.transforms import apply_transform\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import AsDiscrete\n",
    "from monai.data import DataLoader, Dataset, decollate_batch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.unetr UNETR.__init__:pos_embed: Argument `pos_embed` has been deprecated since version 1.2. It will be removed in version 1.4. please use `proj_type` instead.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from collections.abc import Sequence\n",
    "\n",
    "from unetr import CustomedUNETR\n",
    "# Load the pretrained weights\n",
    "#pretrained_weights = torch.load('/home/nada.saadi/CTPET/hecktor2022_cropped/4centers-ctonly/4centers-ctonly.pth')\n",
    "\n",
    "\n",
    "model_ct = UNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    img_size=(96, 96, 96),\n",
    "    feature_size=16,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072, \n",
    "    num_heads=12,\n",
    "    pos_embed=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    "model_pet = UNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    img_size=(96, 96, 96),\n",
    "    feature_size=16,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072, \n",
    "    num_heads=12,\n",
    "    pos_embed=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNETR(\n",
       "  (vit): ViT(\n",
       "    (patch_embedding): PatchEmbeddingBlock(\n",
       "      (patch_embeddings): Sequential(\n",
       "        (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)\n",
       "        (1): Linear(in_features=4096, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x TransformerBlock(\n",
       "        (mlp): MLPBlock(\n",
       "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (fn): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): SABlock(\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (proj_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (proj_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (input_rearrange): Rearrange('b h (l d) -> b l h d', l=12)\n",
       "          (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "          (drop_output): Dropout(p=0.0, inplace=False)\n",
       "          (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (encoder1): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder2): UnetrPrUpBlock(\n",
       "    (transp_conv_init): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x Sequential(\n",
       "        (0): Convolution(\n",
       "          (conv): ConvTranspose3d(32, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "        )\n",
       "        (1): UnetResBlock(\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder3): UnetrPrUpBlock(\n",
       "    (transp_conv_init): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Convolution(\n",
       "          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "        )\n",
       "        (1): UnetResBlock(\n",
       "          (conv1): Convolution(\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (conv2): Convolution(\n",
       "            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          )\n",
       "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "          (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder4): UnetrPrUpBlock(\n",
       "    (transp_conv_init): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (blocks): ModuleList()\n",
       "  )\n",
       "  (decoder5): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder4): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder3): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder2): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (out): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(16, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ct.load_state_dict(torch.load(ct_model_7))\n",
    "model_pet.load_state_dict(torch.load(pt_model_7))\n",
    "\n",
    "    \n",
    "\n",
    "model_ct.load_state_dict(torch.load(ct_model_7), strict=False)\n",
    "model_pet.load_state_dict(torch.load(pt_model_7), strict=False)\n",
    "\n",
    "model_ct.eval() \n",
    "model_pet.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_data_7='/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_CT_train_new.json'\n",
    "Pt_data_7='/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_PET_train_new.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created at /home/nada.saadi/CTPET/hecktor2022_cropped/HMR_CT_train_new.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "def generate_paths(patient_id):\n",
    "    base_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data'\n",
    "    return {\n",
    "        'id': os.path.join(base_dir, patient_id),\n",
    "        'ct': os.path.join(base_dir, patient_id, f\"{patient_id}_ct.nii.gz\"),\n",
    "        'seg': os.path.join(base_dir, patient_id, f\"{patient_id}_gt.nii.gz\")\n",
    "    }\n",
    "\n",
    "# Assign each data entry to a random fold\n",
    "all_data = []\n",
    "num_folds = 5\n",
    "\n",
    "for file_dir in sorted(glob('data/*')):\n",
    "    patient_id = file_dir.split('/')[-1]\n",
    "    # Check if the file belongs to the MDA center\n",
    "    if patient_id.startswith(\"HMR-\"):\n",
    "        entry = generate_paths(patient_id)\n",
    "        entry['fold'] = random.randint(1, num_folds) - 1\n",
    "        all_data.append(entry)\n",
    "\n",
    "# Compile data into a JSON structure\n",
    "data_json = {\"training\": all_data}\n",
    "\n",
    "# Save to JSON file\n",
    "json_file_path = \"/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_CT_train_new.json\"\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(data_json, f, indent=4)\n",
    "\n",
    "print(f\"JSON file created at {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in HMR CT samples: 17\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "hmr_ct_files = glob.glob('/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-*/HMR-*_ct.nii.gz')\n",
    "num_files = len(hmr_ct_files)\n",
    "\n",
    "print(f\"Number of files in HMR CT samples: {num_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped'\n",
    "json_dir_ct = \"/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_CT_train_new.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datafold_read(datalist, basedir, fold=0, key=\"training\"):\n",
    "    with open(datalist) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    json_data = json_data[key]\n",
    "\n",
    "    for d in json_data:\n",
    "        for k in d:\n",
    "            if isinstance(d[k], list):\n",
    "                d[k] = [os.path.join(basedir, iv) for iv in d[k]]\n",
    "            elif isinstance(d[k], str):\n",
    "                d[k] = os.path.join(basedir, d[k]) if len(d[k]) > 0 else d[k]\n",
    "\n",
    "    tr = []\n",
    "    val = []\n",
    "    for d in json_data:\n",
    "        if \"fold\" in d and d[\"fold\"] == fold:\n",
    "            val.append(d)\n",
    "        else:\n",
    "            tr.append(d)\n",
    "\n",
    "    return tr, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation files have been saved to /home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_validation_files.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "train_files, validation_files = datafold_read(datalist=json_dir_ct, basedir=data_dir, fold=4)\n",
    "len(train_files), len(validation_files)\n",
    "# Path for the new JSON file\n",
    "ct_validation_json_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_validation_files.json'\n",
    "\n",
    "# Save the validation files to a JSON file\n",
    "with open(ct_validation_json_path, 'w') as file:\n",
    "    json.dump(validation_files, file, indent=4)\n",
    "\n",
    "print(f\"Validation files have been saved to {ct_validation_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_files)\n",
    "hmr_ct_validation_files = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_validation_files.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of elements in hmr_ct_validation_files is: 4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_validation_files.json'\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Count the number of elements\n",
    "num_elements = len(data)\n",
    "\n",
    "# Print the number of elements\n",
    "print(f\"The number of elements in hmr_ct_validation_files is: {num_elements}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_ct.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_gt.nii.gz', 'fold': 4}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_ct.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_gt.nii.gz', 'fold': 4}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_ct.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_gt.nii.gz', 'fold': 4}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_ct.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_gt.nii.gz', 'fold': 4}]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_validation_files.json', 'r') as file:\n",
    "    hmr_ct_validation_files = json.load(file)\n",
    "\n",
    "print(hmr_ct_validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created at /home/nada.saadi/CTPET/hecktor2022_cropped/HMR_PET_train_new.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "def generate_paths(patient_id):\n",
    "    base_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data'\n",
    "    return {\n",
    "        'id': os.path.join(base_dir, patient_id),\n",
    "        'pt': os.path.join(base_dir, patient_id, f\"{patient_id}_pt.nii.gz\"),\n",
    "        'seg': os.path.join(base_dir, patient_id, f\"{patient_id}_gt.nii.gz\")\n",
    "    }\n",
    "\n",
    "# Assign each data entry to a random fold\n",
    "all_data = []\n",
    "num_folds = 5\n",
    "\n",
    "for file_dir in sorted(glob('data/*')):\n",
    "    patient_id = file_dir.split('/')[-1]\n",
    "    # Check if the file belongs to the MDA center\n",
    "    if patient_id.startswith(\"HMR-\"):\n",
    "        entry = generate_paths(patient_id)\n",
    "        entry['fold'] = random.randint(1, num_folds) - 1\n",
    "        all_data.append(entry)\n",
    "\n",
    "# Compile data into a JSON structure\n",
    "data_json = {\"training\": all_data}\n",
    "\n",
    "# Save to JSON file\n",
    "json_file_path = \"/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_PET_train_new.json\"\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(data_json, f, indent=4)\n",
    "\n",
    "print(f\"JSON file created at {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped'\n",
    "json_dir_pt = \"/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_PET_train_new.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datafold_read(datalist, basedir, fold=0, key=\"training\"):\n",
    "    with open(datalist) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    json_data = json_data[key]\n",
    "\n",
    "    for d in json_data:\n",
    "        for k in d:\n",
    "            if isinstance(d[k], list):\n",
    "                d[k] = [os.path.join(basedir, iv) for iv in d[k]]\n",
    "            elif isinstance(d[k], str):\n",
    "                d[k] = os.path.join(basedir, d[k]) if len(d[k]) > 0 else d[k]\n",
    "\n",
    "    tr = []\n",
    "    val = []\n",
    "    for d in json_data:\n",
    "        if \"fold\" in d and d[\"fold\"] == fold:\n",
    "            val.append(d)\n",
    "        else:\n",
    "            tr.append(d)\n",
    "\n",
    "    return tr, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation files have been saved to /home/nada.saadi/CTPET/hecktor2022_cropped/hmr_pt_validation_files.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "train_files, validation_files = datafold_read(datalist=json_dir_pt, basedir=data_dir, fold=2)\n",
    "len(train_files), len(validation_files)\n",
    "# Path for the new JSON file\n",
    "validation_json_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_pt_validation_files.json'\n",
    "\n",
    "# Save the validation files to a JSON file\n",
    "with open(validation_json_path, 'w') as file:\n",
    "    json.dump(validation_files, file, indent=4)\n",
    "\n",
    "print(f\"Validation files have been saved to {validation_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in HMR PET samples: 17\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "hmr_pt_files = glob.glob('/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-*/HMR-*_pt.nii.gz')\n",
    "num_files = len(hmr_pt_files)\n",
    "\n",
    "print(f\"Number of files in HMR PET samples: {num_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmr_pt_validation_files='/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_pt_validation_files.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-001', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-001/HMR-001_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-001/HMR-001_gt.nii.gz', 'fold': 2}\n",
      "{'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_gt.nii.gz', 'fold': 2}\n",
      "{'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_gt.nii.gz', 'fold': 2}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = hmr_pt_validation_files\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Print the first 5 elements\n",
    "for i in range(3):\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified CT validation files have been saved to /home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_modified_validation_files.json\n",
      "Modified PET validation files have been saved to /home/nada.saadi/CTPET/hecktor2022_cropped/hmr_pt_modified_validation_files.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the existing JSON files\n",
    "ct_json_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_validation_files.json'\n",
    "pet_json_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_pt_validation_files.json'\n",
    "\n",
    "# Path for the new JSON files\n",
    "ct_new_json_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ct_modified_validation_files.json'\n",
    "pet_new_json_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_pt_modified_validation_files.json'\n",
    "\n",
    "# Read the existing JSON files\n",
    "with open(ct_json_path, 'r') as ct_file, open(pet_json_path, 'r') as pet_file:\n",
    "    ct_data = json.load(ct_file)\n",
    "    pet_data = json.load(pet_file)\n",
    "\n",
    "# Remove the 'id' and 'fold' keys from the data\n",
    "ct_modified_data = [{k: v for k, v in item.items() if k not in ['id', 'fold']} for item in ct_data]\n",
    "pet_modified_data = [{k: v for k, v in item.items() if k not in ['id', 'fold']} for item in pet_data]\n",
    "\n",
    "# Save the modified data to new JSON files\n",
    "with open(ct_new_json_path, 'w') as ct_new_file, open(pet_new_json_path, 'w') as pet_new_file:\n",
    "    json.dump(ct_modified_data, ct_new_file, indent=4)\n",
    "    json.dump(pet_modified_data, pet_new_file, indent=4)\n",
    "\n",
    "print(f\"Modified CT validation files have been saved to {ct_new_json_path}\")\n",
    "print(f\"Modified PET validation files have been saved to {pet_new_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipCT(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on hecktor classes:\n",
    "    label 1 is the tumor\n",
    "    label 2 is the lymph node\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            if key == \"ct\":\n",
    "                d[key] = torch.clip(d[key], min=-200, max=200)\n",
    "            # elif key == \"pt\":\n",
    "            #     d[key] = torch.clip(d[key], d[key].min(), 5)\n",
    "        return d\n",
    "\n",
    "class MulPTFM(MapTransform):\n",
    "    \"\"\"\n",
    "    Mult PT and FM \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "\n",
    "        fm = d[\"ct\"] > 0\n",
    "        d[\"pt\"] = d[\"pt\"] * fm\n",
    "        return d\n",
    "\n",
    "class SelectClass(MapTransform):\n",
    "    \"\"\"\n",
    "    Select the class for which you want to fine tune the model \n",
    "\n",
    "    \"\"\"\n",
    "    # def __init__(self, keys, cls=1):\n",
    "    #     super(self).__init__(keys)\n",
    "    #     self.cls = cls\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        d[\"seg\"][d[\"seg\"] == 1] = 0\n",
    "        # d[\"seg\"][d[\"seg\"] == 2] = 1\n",
    "        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transforms_ct = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"ct\",  \"seg\"], ensure_channel_first = True),\n",
    "        SpatialPadd(keys=[\"ct\",  \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "        Orientationd(keys=[\"ct\",  \"seg\"], axcodes=\"PLS\"),\n",
    "        #NormalizeIntensityd(keys=[\"pt\"]),\n",
    "        ClipCT(keys=[\"ct\"]),\n",
    "        ScaleIntensityd(keys=[\"ct\",], minv=0, maxv=1),\n",
    "        #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "        ConcatItemsd(keys=[ \"ct\"], name=\"ct\"),\n",
    "    ]\n",
    ")\n",
    "def create_dataloader(data, transforms, batch_size=2, shuffle=True):\n",
    "    # Create CacheDataset with the reformatted data\n",
    "    dataset = Dataset(data=data, transform=transforms)\n",
    "\n",
    "    # Create DataLoader\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=8)\n",
    "val_loader = create_dataloader(ct_modified_data, val_transforms_ct, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transforms_pt = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[ \"pt\", \"seg\"], ensure_channel_first = True),\n",
    "        SpatialPadd(keys=[ \"pt\", \"seg\"], spatial_size=(200, 200, 310), method='end'),\n",
    "        Orientationd(keys=[ \"pt\", \"seg\"], axcodes=\"PLS\"),\n",
    "        NormalizeIntensityd(keys=[\"pt\"]),\n",
    "        ClipCT(keys=[\"pt\"]),\n",
    "        ScaleIntensityd(keys=[\"pt\"], minv=0, maxv=1),\n",
    "        #MulPTFM(keys=[\"ct\",\"pt\"]),\n",
    "        ConcatItemsd(keys=[\"pt\"], name=\"pt\"),\n",
    "    ]\n",
    ")\n",
    "def create_dataloader(data, transforms, batch_size=2, shuffle=True):\n",
    "    # Create CacheDataset with the reformatted data\n",
    "    dataset = Dataset(data=data, transform=transforms)\n",
    "\n",
    "    # Create DataLoader\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=8)\n",
    "val_loader = create_dataloader(pet_modified_data, val_transforms_pt, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "with open(ct_json_path, 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=Dataset(data=test_data, transform=val_transforms_ct),\n",
    "    batch_size=1,  # Batch size for testing can be 1 since no backpropagation is required\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/nada.saadi/MIS-FM/hecktor2022_cropped'\n",
    "json_dir_ctpt = \"/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_ctpt_train_new.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified CT validation files have been saved to /home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ctpt_modified_validation_files.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the existing JSON files\n",
    "# Check if ctpt_data is a list of dictionaries\n",
    "if isinstance(ctpt_data, list) and all(isinstance(item, dict) for item in ctpt_data):\n",
    "    # Remove the 'id' and 'fold' keys from the data\n",
    "    ctpt_modified_data = [{k: v for k, v in item.items() if k not in ['id', 'fold']} for item in ctpt_data]\n",
    "else:\n",
    "    print(\"ctpt_data is not a list of dictionaries.\")\n",
    "\n",
    "\n",
    "# Save the modified data to new JSON files\n",
    "with open(ctpt_new_json_path, 'w') as ct_new_file, open(ctpt_new_json_path, 'w') as ctpt_new_file:\n",
    "    json.dump(ctpt_modified_data, ct_new_file, indent=4)\n",
    "  \n",
    "    # Check if ctpt_data is a list of dictionaries\n",
    "    if not isinstance(ctpt_data, list) or not all(isinstance(item, dict) for item in ctpt_data):\n",
    "        ctpt_data = [ctpt_data]  # Convert single dictionary to a list of dictionaries\n",
    "\n",
    "print(f\"Modified CT validation files have been saved to {ctpt_new_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'training': [{'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-001',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-001/HMR-001_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-001/HMR-001_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-001/HMR-001_gt.nii.gz',\n",
       "    'fold': 3},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-004',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-004/HMR-004_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-004/HMR-004_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-004/HMR-004_gt.nii.gz',\n",
       "    'fold': 4},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-005',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-005/HMR-005_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-005/HMR-005_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-005/HMR-005_gt.nii.gz',\n",
       "    'fold': 1},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-011',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-011/HMR-011_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-011/HMR-011_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-011/HMR-011_gt.nii.gz',\n",
       "    'fold': 0},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-012',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-012/HMR-012_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-012/HMR-012_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-012/HMR-012_gt.nii.gz',\n",
       "    'fold': 0},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_gt.nii.gz',\n",
       "    'fold': 2},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-016',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-016/HMR-016_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-016/HMR-016_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-016/HMR-016_gt.nii.gz',\n",
       "    'fold': 3},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_gt.nii.gz',\n",
       "    'fold': 2},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_gt.nii.gz',\n",
       "    'fold': 1},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-023',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-023/HMR-023_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-023/HMR-023_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-023/HMR-023_gt.nii.gz',\n",
       "    'fold': 1},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-024',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-024/HMR-024_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-024/HMR-024_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-024/HMR-024_gt.nii.gz',\n",
       "    'fold': 4},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-025',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-025/HMR-025_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-025/HMR-025_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-025/HMR-025_gt.nii.gz',\n",
       "    'fold': 0},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-028',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-028/HMR-028_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-028/HMR-028_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-028/HMR-028_gt.nii.gz',\n",
       "    'fold': 1},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_gt.nii.gz',\n",
       "    'fold': 2},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-030',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-030/HMR-030_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-030/HMR-030_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-030/HMR-030_gt.nii.gz',\n",
       "    'fold': 1},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-034',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-034/HMR-034_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-034/HMR-034_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-034/HMR-034_gt.nii.gz',\n",
       "    'fold': 0},\n",
       "   {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040',\n",
       "    'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040/HMR-040_ct.nii.gz',\n",
       "    'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040/HMR-040_pt.nii.gz',\n",
       "    'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040/HMR-040_gt.nii.gz',\n",
       "    'fold': 2}]}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctpt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Assuming the structure is always like the one provided,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# where ctpt_data is a list containing a single dictionary,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# and 'training' is a key in that dictionary:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Access the 'training' key in the first dictionary of the list\u001b[39;00m\n\u001b[1;32m     13\u001b[0m training_data \u001b[38;5;241m=\u001b[39m ctpt_data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m val_ctpt_data \u001b[38;5;241m=\u001b[39m \u001b[43mctpt_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Count the elements in training_data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m training_data_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(training_data)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation'"
     ]
    }
   ],
   "source": [
    "# Specify the path to the JSON file\n",
    "json_file_path = ctpt_new_json_path\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    ctpt_data = json.load(json_file)\n",
    "\n",
    "# Assuming the structure is always like the one provided,\n",
    "# where ctpt_data is a list containing a single dictionary,\n",
    "# and 'training' is a key in that dictionary:\n",
    "\n",
    "# Access the 'training' key in the first dictionary of the list\n",
    "training_data = ctpt_data[0]['training']\n",
    "val_ctpt_data = ctpt_data[0]['validation']\n",
    "# Count the elements in training_data\n",
    "training_data_count = len(training_data)\n",
    "val_data_count = len(val_ctpt_data)\n",
    "\n",
    "# Print the count\n",
    "print(\"Number of samples in 'training':\", training_data_count)\n",
    "print(\"Number of samples in 'validation':\", val_data_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation files: 5\n",
      "Validation data saved to hmr_ctpt_val.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "json_file_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/hmr_ctpt_modified_validation_files.json'  # Adjust this to your actual file path\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    ctpt_data = json.load(json_file)\n",
    "\n",
    "# Filter samples where 'fold' is 2 to create a validation set\n",
    "validation_data = []\n",
    "for group in ctpt_data:\n",
    "    for sample in group['training']:\n",
    "        if sample['fold'] == 2:\n",
    "            validation_data.append(sample)\n",
    "\n",
    "# Count the number of validation files\n",
    "validation_count = len(validation_data)\n",
    "print(\"Number of validation files:\", validation_count)\n",
    "\n",
    "# Save the filtered data to a new JSON file\n",
    "new_json_file_path = 'hmr_ctpt_val.json'  # Name of the new JSON file\n",
    "with open(new_json_file_path, \"w\") as new_json_file:\n",
    "    json.dump(validation_data, new_json_file, indent=4)\n",
    "\n",
    "print(f\"Validation data saved to {new_json_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_ct.nii.gz', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_gt.nii.gz', 'fold': 2}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_ct.nii.gz', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_gt.nii.gz', 'fold': 2}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_ct.nii.gz', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_gt.nii.gz', 'fold': 2}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_ct.nii.gz', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_gt.nii.gz', 'fold': 2}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040/HMR-040_ct.nii.gz', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040/HMR-040_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040/HMR-040_gt.nii.gz', 'fold': 2}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "json_file_path = new_json_file_path\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Print the data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datafold_read(datalist, basedir, fold=0, key=\"training\"):\n",
    "    with open(datalist) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    json_data = json_data[key]\n",
    "\n",
    "    for d in json_data:\n",
    "        for k in d:\n",
    "            if isinstance(d[k], list):\n",
    "                d[k] = [os.path.join(basedir, iv) for iv in d[k]]\n",
    "            elif isinstance(d[k], str):\n",
    "                d[k] = os.path.join(basedir, d[k]) if len(d[k]) > 0 else d[k]\n",
    "\n",
    "    tr = []\n",
    "    val = []\n",
    "    for d in json_data:\n",
    "        if \"fold\" in d and d[\"fold\"] == fold:\n",
    "            val.append(d)\n",
    "        else:\n",
    "            tr.append(d)\n",
    "\n",
    "    return tr, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 4)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files_ctpt, validation_files_ctpt = datafold_read(datalist=json_dir, basedir=data_dir, fold=0)\n",
    "len(train_files_ctpt), len(validation_files_ctpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the path of the new JSON file\n",
    "new_json_path = \"/path/to/new_json_file.json\"\n",
    "\n",
    "# Save the ctpt validation files to the new JSON file\n",
    "with open(new_json_path, \"w\") as f:\n",
    "    json.dump(ctpt_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the path of the previous validation files JSON\n",
    "previous_json_path = \"/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_ctpt_train_new.json\"\n",
    "\n",
    "# Define the path of the new JSON file\n",
    "new_json_path = \"/home/nada.saadi/CTPET/hecktor2022_cropped/val_ctpt_hmr.json\"\n",
    "\n",
    "# Load the previous validation files JSON\n",
    "with open(previous_json_path, \"r\") as f:\n",
    "    previous_json = json.load(f)\n",
    "\n",
    "# Save the previous validation files as a new JSON file\n",
    "with open(new_json_path, \"w\") as f:\n",
    "    json.dump(previous_json, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_ct.nii.gz', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-013/HMR-013_gt.nii.gz', 'fold': 2}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_ct.nii.gz', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-020/HMR-020_gt.nii.gz', 'fold': 2}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_ct.nii.gz', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-021/HMR-021_gt.nii.gz', 'fold': 2}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_ct.nii.gz', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-029/HMR-029_gt.nii.gz', 'fold': 2}, {'id': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040', 'ct': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040/HMR-040_ct.nii.gz', 'pt': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040/HMR-040_pt.nii.gz', 'seg': '/home/nada.saadi/MIS-FM/hecktor2022_cropped/data/HMR-040/HMR-040_gt.nii.gz', 'fold': 2}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "json_file_path = new_json_file_path\n",
    "\n",
    "# Read the JSON file\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    data_ctpt = json.load(json_file)\n",
    "\n",
    "# Print the data\n",
    "print(data_ctpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/monai/data/dataset.py\", line 112, in __getitem__\n    return self._transform(index)\n  File \"/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/monai/data/dataset.py\", line 97, in _transform\n    data_i = self.data[index]\nKeyError: 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_dice_test, metric_tumor, metric_lymph\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Save the weights\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtesting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[99], line 32\u001b[0m, in \u001b[0;36mtesting\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m dice_metric_batch_fn\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader):\n\u001b[1;32m     33\u001b[0m         test_inputs, test_labels \u001b[38;5;241m=\u001b[39m batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mct\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda(), batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     35\u001b[0m         test_outputs_ct \u001b[38;5;241m=\u001b[39m sliding_window_inference(test_inputs, (\u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m), \u001b[38;5;241m4\u001b[39m, model_ct)\n",
      "File \u001b[0;32m~/miniconda3/envs/clam/lib/python3.8/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/clam/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/clam/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/clam/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/clam/lib/python3.8/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/monai/data/dataset.py\", line 112, in __getitem__\n    return self._transform(index)\n  File \"/home/nada.saadi/miniconda3/envs/clam/lib/python3.8/site-packages/monai/data/dataset.py\", line 97, in _transform\n    data_i = self.data[index]\nKeyError: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=Dataset(data=test_data, transform=val_transforms_ct),\n",
    "    batch_size=1,  # Batch size for testing can be 1 since no backpropagation is required\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.AdamW(model_ct.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "post_label = AsDiscrete(to_onehot=3)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=3)\n",
    "\n",
    "# Dice metric for evaluation\n",
    "dice_metric_fn = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "dice_metric_batch_fn = DiceMetric(include_background=False, reduction=\"mean_batch\")\n",
    "\n",
    "#pt_weights='/home/nada.saadi/CTPET/hecktor2022_cropped/4centers-ctonly/4centers-ctonly.pth'\n",
    "# pt_weights_5='/home/nada.saadi/CTPET/hecktor2022_cropped/5th_ctpt_tokens_ourapproach/5th_center_our_approach.pth'\n",
    "# state_dict = torch.load(pt_weights_5)\n",
    "# model_state_dict = model_ct.state_dict()\n",
    "# for name, param in state_dict.items():\n",
    "#     if name in model_state_dict:\n",
    "#         if param.shape == model_state_dict[name].shape:\n",
    "#             model_state_dict[name] = param\n",
    "\n",
    "def testing():\n",
    "    model_ct.eval()\n",
    "    dice_metric_fn.reset()\n",
    "    dice_metric_batch_fn.reset()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(test_loader):\n",
    "            test_inputs, test_labels = batch_data[\"ct\"].cuda(), batch_data[\"seg\"].cuda()\n",
    "            \n",
    "            test_outputs_ct = sliding_window_inference(test_inputs, (96, 96, 96), 4, model_ct)\n",
    "            \n",
    "            # Convert outputs and labels to one-hot format for DiceMetric calculation\n",
    "            test_outputs_ct = [AsDiscrete(argmax=True, to_onehot=3)(i) for i in decollate_batch(test_outputs_ct)]\n",
    "            test_labels = [AsDiscrete(to_onehot=3)(i) for i in decollate_batch(test_labels)]\n",
    "\n",
    "            dice_metric_fn(y_pred=test_outputs_ct, y=test_labels)\n",
    "            dice_metric_batch_fn(y_pred=test_outputs_ct, y=test_labels)\n",
    "            \n",
    "        # After processing all batches, save the outputs\n",
    "    #ct_save_path = '/home/nada.saadi/CTPET/hecktor2022_cropped/test_output_ct_saved.pt'\n",
    "    ct_save_path_6thcenter = '/home/nada.saadi/CTPET/hecktor2022_cropped/HMR_test_output_ct_saved.pt'\n",
    "    torch.save(test_outputs_ct, ct_save_path_6thcenter)\n",
    "    print(f\"Outputs saved at: {ct_save_path_6thcenter}\")\n",
    "    \n",
    "\n",
    "    mean_dice_test = dice_metric_fn.aggregate().item()\n",
    "    metric_batch_test = dice_metric_batch_fn.aggregate()\n",
    "    metric_tumor = metric_batch_test[0].item()\n",
    "    metric_lymph = metric_batch_test[1].item()\n",
    "\n",
    "    print(f\"Testing - Avg Dice: {mean_dice_test:.4f}, Tumor Dice: {metric_tumor:.4f}, Lymph Dice: {metric_lymph:.4f}\")\n",
    "    return mean_dice_test, metric_tumor, metric_lymph\n",
    "    # Save the weights\n",
    "testing() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
